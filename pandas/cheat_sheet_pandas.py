#!/usr/bin/env python
# coding: utf-8

# # Шпаргалка - `pandas`
# 
# Это ключевая библиотека для работы с данными. Данная библиотека содержит очень большую функциональность и можно долго узнавать все ее возможности, но нашей задачей сейчас является - узнать наиболее важные функции, которые помогут дальше работать с данными.
# 
# ## Видео шаг за шагом (дополнительная поддержка для начинающих)

# In[5]:


get_ipython().run_cell_magic('html', '', '<iframe style="height:500px;width:100%" src="https://www.youtube.com/embed/Yug7QHkt0qU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>')


# Нам для начала нужно проимпортировать сами библиотеки `pandas` и `numpy` (данная библиотека активно используется в самом `pandas`).

# In[1]:


import pandas as pd
import numpy as np


# Библиотека pandas была создана как эффективное представление данных в виде серии (англ. "Series") или таблицы данных (анг. "DataFrame"). 
# 
# ## Series 
# Давай создадим простую серию (или вектор), которая будет состоять из двух строчек. *Обрати внимание*, что каждая строка имеет индекс. В  случае первой строки - `row1` - это индекс, а `row1 value` - значение.

# In[2]:


sq = pd.Series({
    'row1': 'row1 value', 
    'row2': 'row2 value'
})
sq


# `Series` - это одноколоночные или векторные данные. У каждой серии есть значения и индексы. Мы можем использовать `.index` для доступа к индексам. Зачем нам индексы? Это введено с целью оптимизации. Для начала (если это звучит трудно), не нужно сильно в это вникать - просто запомни, что благодаря индексам эта вся структура работает быстрее.

# In[3]:


sq.index


# In[4]:


sq.values


# ## DataFrame
# 
# Как упоминалось ранее, `Series` - это один столбец. Если мы хотим загрузить данные в виде «таблицы» (например, файла Excel), нам на помощь приходит `DataFrame`. Говоря простым языком, в терминологии `pandas` таблицу называют `DataFrame`.
# 
# Создадим таблицу из трех столбцов и двух строк.

# In[6]:


df = pd.DataFrame(
    {
     'column_a': {'row1': 'row1 col a', 'row2': 'row2 col a'}, 
     'column_b': {'row1': 'row1 col b', 'row2': 'row2 col b'}, 
     'column_c': {'row1': 'row1 col c', 'row2': 'row2 col c'}, 
    })
df


# Теперь давай разберем DataFrame на простые элементы:
# 
# ![](../images/df.png)
# 
# - **индекс** (зеленая зона) - грубо говоря - это номера строк (но там могут быть и не только номера)
# - **столбцы** (красная область) - названия наших столбцов
# - **значения** (синяя область) - содержимое таблицы

# ### Столбцы
# 
# Предположим, мы хотим выбрать из всей таблицы второй столбец, название которого `column_b`. Это можно сделать легко следующим образом:

# In[7]:


df['column_b']


# Таким образом видим все значения (для всех строк) в столбце `column_b` и дополнительно еще видим индекс (`row1` и `row2` в данном случае).
# 
# 
# Используя `.columns` можно проверить, какие столбцы (их названия в виде списка) есть в ` DataFrame`.

# In[8]:


df.columns


# Видим, что в нашей таблице (анг. `DataFame`) три столбца: `['column_a', 'column_b', 'column_c']`.

# Если возникла необходимость переименовать столбцы, есть несколько способов сделать это. Довольно простой: нужно написать новым списком `df.columns`.

# In[9]:


df.columns = ['new_column1', 'new_column2', 'new_column3'] 
df


# Иногда возникает такая ситуация, что хотим переименовать только один столбец, это можно легко сделать вот так:

# In[10]:


df.rename(columns={'new_column1': 'abc'})


# Теперь давай разберем наиболее интересные функции, которые могут пригодиться. Немного упрощая можно поделить наиболее необходимые функции на 7 групп.
# 
# ### 1. Импорт данных
# - `.read_hdf()`
# - `.read_csv()`
# 
# ### 2. Просмотр данных
# - `.shape()`
# - `.head()`
# - `.sample()`
# - `.tail()`
# - `.info()`
# 
# ### 3. Трансформация/изменения данных
# - `.map()`
# - `.apply()`
# - `.fillna()`
# - `.factorize()`
# 
# ### 4. Сортировка и группировка данных
# - `.value_counts()`
# - `.groupby()`
# - `.agg()`
# - `.sort_values()`
# - `.pivot_table()`
# 
# ### 5. Статистические функции
# - `.mean()`
# - `.median()`
# - `.min()`
# - `.max()`
# 
# ### 6. Визуализация
# - `.hist()`
# 
# 
# ### 7. Другие функции
# - `.qcut()` 
# - `.unique()`
# - `.nunique()`
# 
# 
# Сейчас на конкретном примере проработаем все эти функции.

# Теперь импортируем данные. В нашем случае они записаны в формате - [hdf](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), который иногда называют h5 (имеется ввиду hdf5).

# In[11]:


df = pd.read_hdf("../input/train_data.h5")


# Чтобы проверить количество строк и колонок, можно использовать `.shape` (кстати, обрати внимание, что `.shape` без скобочек).

# In[12]:


df.shape


# Получается у нас **22 732** строчки и **8** столбцов.
# 
# Если мы хотим посмотреть, например, первые 5 строк, используем `.head()`.

# In[13]:


df.head()


# Можно посмотреть 5 случайных строк, используя `.sample()`.

# In[14]:


df.sample(5)


# Можно посмотреть 5 последних строчек, используя функцию `.tail()`.

# In[15]:


df.tail()


# При помощи функции `.info()` можно посмотреть информацию о столбцах:
# - название столбца
# - тип данных (то как `pandas` распознал и это потом можно пробовать менять)
# - количество непустых значений (анлk. `non null`) для каждого столбца
# - количество памяти (нижняя граница), которую занимают эти данные

# In[16]:


df.info()


# ## 3. Трансформация/изменение данных
# - `.map()`
# - `.apply()`
# - `.fillna()`
# - `.factorize()`

# Начнем с функции `.map()`. Она часто появляется. Идея достаточно простая, у нас есть столбец с некоторыми значениями. Далее мы хотим пройти значение за значением и создать новое, на основании определенной логики.
# 
# Лучше всего показать на примере. Давай посмотрим на значения в столбце `"price"`.

# In[17]:


df["price"]


# Видно, что значения написаны в рублях `₽`. Нам нужно из этих значений сделать числа.
# 
# Было `5 402 084 ₽` (как строчка) и должно быть `5402084` (как число). Как это можно сделать? Нам нужно создать логику, которая со строчки вытянет число. Есть разные способы. Давай выберем простой. Для начала нам нужно избавиться от нечисловых знаков, например, `₽`. Для этого можно использовать функцию `.split()`. И она поделит нашу строчку на два компонента до `₽` и после. То, что "до"" - это число.

# In[18]:


"5 402 084 ₽".split("₽")


# In[19]:


"5 402 084 ₽".split("₽")[0]


# Теперь еще нужно удалить пробелы - для этого можно использовать `.replace()`.

# In[20]:


"5 402 084 ₽".split("₽")[0].replace(" ", "")


# Обрати внимание, что это строка (об этом говорит `'` в начале и в конце). Теперь осталось только сконвертировать это значение в число (`int`).

# In[21]:


int("5 402 084 ₽".split("₽")[0].replace(" ", ""))


# Вот таким образом из оригинальной строки `"5 402 084 ₽"` мы получили число  `5402084`. Теперь еще осталось выполнить эту логика для всех значений в столбце `price`. И тут как раз нам поможет функция `.map()`.

# In[23]:


df1 = df["price"].map(lambda x: int(x.split("₽")[0].replace(" ", "")))
df1.sample(5)


# Тут еще появилась анонимная функция - `lambda`. Но можно сделать обычную функцию с именем и назвать ее, например, `.parse_price()`

# In[24]:


def parse_price(x):
    return int(x.split("₽")[0].replace(" ", ""))


# И теперь можно это применить таким образом:

# In[25]:


df["price"].map(parse_price)


# Функция `.apply()` - это более расширенная функция, чем `.map()`. И это дает возможность иметь доступ одновременно к нескольким столбцам. Но важно помнить про `axis=1`. Ниже пример.

# In[26]:


df.apply(lambda row: row["geo_block"][0] + " => " + row["price"], axis=1)


# ## Отсутствующие значения
# 
# Есть разные стратегии, что делать с пустыми значениями. Весьма хороший подход использовать функцию `.fillna()`. Например, мы часто будем использовать простой трик, заменяя пустые значения числом -1.

# In[27]:


df = df.fillna(-1)


# ## Конвертация данных в числа
# 
# Алгоритмы машинного обучения (которые будем использовать) "ждут" данные в виде чисел, т.е. строки, даты и другие форматы данных нужно как-то преобразовать в числовые. Существуют разные подходы - но опять же, простой и прагматичный подход - это использовать функцию `.factorize()`.
# 
# 
# Как это работает на деле?
# 
# Лучше всего взять какой-нибудь пример. Давай посмотрим на столбец `geo_block`.

# In[28]:


df["geo_block"]


# Здесь у нас список. Используя функцию `.map()`, можем, например, достать первое значение.

# In[29]:


df["geo_block"].map(lambda x: x[0])


# Теперь давай "закодируем" каждому уникальному значению свой уникальный идентификатор.

# In[30]:


ids, labels = df["geo_block"].map(lambda x: x[0]).factorize()
ids, labels


# `.factorize()` возвращает два значения - ID и этикетки (англ. `labels`).

# ### Что здесь получилось?
# - `'г. Москва'` получило `id=0`, иными словами, везде, где будет значение `'г. Москва'` оно заменится на 0.
# - `'Новая Москва'` получило `id=1`, т.е. везде где будет значение `'Новая Москва'` оно заменится на 1.
# - и т.д.
# 
# Почему именно `'г. Москва'` получило значение 0,  а не 10? На самом деле это не так важно. Просто `.factorize()` работает таким образом, что поочередно обрабатывает все доступные id. Сразу 0, потом 1 и тд. Получается, что `'г. Москва'` попалась как первое значение - поэтому "забрало" себе 0. 
# 
# Очень часто `.factorize()[0]` т.е. вытягиваем только идентификаторы и приписываем результат в новую колонку, например `geo_block_0_cat`.

# In[31]:


df["geo_block_0_cat"] = df["geo_block"].map(lambda x: x[0]).factorize()[0]
df["geo_block_0_cat"]


# 
# 

# Как видишь, `geo_block_0_cat` - это новый столбец, который содержит уникальные идентификаторы. Кстати, префикс `_cat` значит, что данный столбец создан при помощи `.factorize()`. Почему `_cat` 😻? Это сокращение от `categorical` (категориальные переменные).

# ## 4. Сортировка и группировка данных
# - `.value_counts()`
# - `.groupby()`
# - `.agg()`
# - `.sort_values()`
# - `.pivottable()`

# Начнем с `.value_counts()`. Это очень удобная функция. Возвращаясь к предыдущему примеру, можно подсмотреть, какие уникальные значения в первом блоке `geo_block`. 

# In[32]:


df["geo_block"].map(lambda x: x[0]).value_counts()


# Видно, что 
# - `г. Москва` появляется **19 310**
# - `Новая Москва` появляется **3 021**
# - `г. Зеленоград` появляется **210**
# 
# 
# Очень удобная функция, чтобы "подсмотреть" 👀 какие значения и сколько раз появляются. Значения можно преобразовать в проценты, чтобы понять, например, `г. Москва` - сколько раз появляется в процентном соотношении.

# In[33]:


df["geo_block"].map(lambda x: x[0]).value_counts(normalize=True)


# Видно, что `г. Москва` появляется в 84.94%, `Новая Москва` появляется 13.28% и тд.

# Теперь разберемся с:
# - `.groupby()`
# - `.agg()`
# - `.sort_values()`

# Только сразу нужно приготовить два новых столбца:
# - `price_num` - это числовое значение, поделенное на миллион (чтобы легче было интерпретировать)
# - `geo_block_0` - первое значение в geo_block

# In[34]:


df["price_num"] = df["price"].map(parse_price).map(lambda x: x/1000000)
df["geo_block_0"] = df["geo_block"].map(lambda x: x[0])


# Теперь давай сгруппируем `geo_block_0` по ключу и потом найдем, например, среднее значение по цене. Иными словами можно посмотреть, какая средняя цена в `г. Москва`, `Новая Москва` и т.д.

# In[35]:


df[ ["geo_block_0", "price_num"] ]


# In[36]:


df.groupby("geo_block_0").agg("mean")["price_num"]


# Видно, например, что средняя цена за недвижимость в `Новая Москва` - 7.27 млн. рублей, в `г. Москва` 19.29 млн. рублей. А, например, в п. Рублево средняя стоимость недвижимости 30 млн. рублей.
# 
# 
# Немного неудобно смотреть данные, когда они не отсортированы, а для этого можно использовать `sort_values()`. По умолчанию сортируются данные по возрастанию.

# In[37]:


df.groupby("geo_block_0").agg("mean")["price_num"].sort_values()


# Можно поменять порядок сортировки и сделать ее по убыванию.

# In[38]:


df.groupby("geo_block_0").agg("mean")["price_num"].sort_values(ascending=False)


# Теперь очень хорошо видно (в данных, к которым у нас есть доступ - это не целая картина!) - самая дорогая недвижимость в `п. Рублево` и самая дешевая недвижимость в `г. Зеленоград`.

# Можно добавить больше критериев - агрегирующих функций. Например, минимальное и максимальное значения. Для этого добавляем в функцию `.agg` список функций `.agg(["mean", "min", "max"])`.

# In[39]:


df.groupby("geo_block_0").agg(["mean", "min", "max", len])["price_num"].sort_values(by="mean", ascending=False)


# ## Сводная таблица (англ. `pivot table`)
# 
# Выше мы использовали функцию `groupby`, но есть возможность быстрее достигать некоторые результаты, при помощи сводных таблиц.

# In[ ]:


pd.pivot_table(df, index=["geo_block_0"], values=["price_num"], aggfunc=["mean", "min", "max", len] )


# ### 5. Статистические функции
# - `.min()`
# - `.max()`
# - `.mean()`
# - `.median()`

# Когда у нас список значений, то можно найти минимальные и максимальные значения в этом списке. Например, самая дешевая недвижимость (минимальное значение) и самая дорогая недвижимость (максимальное значение).

# In[ ]:


df["price_num"].min(), df["price_num"].max()


# Из наших данных получается, что самая дешевая недвижимость - это 1 млн. рублей, а самая дорогая - это более 3000 млн рублей (т.е. более 3 млрд. рублей).

# Очень часто, анализируя данные, мы хотим как-то их "уменьшить" до одного числа, чтобы легче было сравнивать. Особенно часто прибегают к "среднему значению". Кстати, стоит заметить что существуют разные средние значения, но по умолчанию (если не уточняется) имеется в виду - [среднее арифметическое значение](https://ru.wikipedia.org/wiki/%D0%A1%D1%80%D0%B5%D0%B4%D0%BD%D0%B5%D0%B5_%D0%B0%D1%80%D0%B8%D1%84%D0%BC%D0%B5%D1%82%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%BE%D0%B5).
# 
# Пример - 5 значений.

# In[ ]:


vals = [1, 4, 2, 3, 2]


# Сумма получается: 1 + 4 + 2 + 3 + 2 = 12.
# Всего элементов (чисел) - 5 штук
# 
# Среднее будет: 12/5 = 2.4

# In[ ]:


np.mean([1, 4, 2, 3, 2])


# Ну хорошо, теперь можно заметить, что в среднем может быть очень большой изъян (чувствительность) на одно большое число. Предположим, что мы говорим о зарплате в компании.
# 
# 
# 9 человек в этой компании зарабатывают - `$100` и один человек зарабатывает `$10 000`.  Какая будет средняя зарплата?

# In[ ]:


np.mean([100] * 9 + [10000])


# Получается, что средняя зарплата больше тысячи долларов в этой компании. Знакомая история?
# 
# 
# "средняя температура по больнице" - это анекдот, который высмеивает ту же самую проблему.
# 
# ```
# — Какова средняя температура больных в энской больнице?
# — 36,6 °С, включая гнойное и морг!
# ```
# «Один бьётся в горячке, другой остывает в морге, а средняя температура по больнице 36,6 °C.» (c)
# 
# 
# Медиана - частично решает вышеперечисленные проблемы, так как работает иначе. Три этапа.
# 1. Сортировка всех значений
# 2. Поиск середины
# 3. Элемент (или среднее пары, сейчас эти нюансы не так важны) и является медианой.
# 
# 
# Например с зарплатой.
# 1. Сортируем зарплату, получается, что первые 9 элементов по 100 и последний 10 000.
# 2. Так как кол-во элементов четное, то берем пару посередине (т.е. пятый и шестой элемент) 
# 3. Находим среднее `(100 + 100) / 2` получается 100
# 
# Медиана в случае зарплаты получается 100. Как это интерпретировать - это как минимум у половины работников зарплата $100  или меньше (и наоборот).

# In[ ]:


np.median([100] * 9 + [10000])


# Давай теперь применим эти знания для цен на недвижимость.

# In[ ]:


df["price_num"].mean(), df["price_num"].median()


# Получается среднее значение вышло более 17 млн. рублей. Но это прежде всего из-за того, что была недвижимость более 3 млрд. рублей (это сильно завышает среднюю).
# 
# Но в тоже время видно, что медиана 9.89 млн. рублей. Получается, что половина недвижимости стоит 9.89 млн рублей, либо меньше (ну и другая половина больше)!

# ### 6. Визуализация
# - `.hist()`

# Посмотреть на данные не только в виде таблички, но также и визуально. Сейчас познакомимся с [гистограммой](https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D1%81%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0).
# 
# Посмотрим на то, как выглядит распределение цен на недвижимость.

# In[ ]:


df["price_num"].hist(bins=100);


# Немного "искажается картина" из-за того, что есть очень дорогая недвижимость (более 3 млрд. рублей). Есть разные способы, как бороться с "отстающими значениями" (англ. `outliers`). Один из самых простых - это "пропустить" через логарифм.

# In[ ]:


np.log10(df["price_num"]).hist(bins=100);


# Видно, что большая часть недвижимости находится около 1. В данном случае 1 - это значит 10 млн. рублей.
# Значение 1.5 - это около 30 млн рублей. А почему столько? 10 в степени 1.5. В Python в степень можно возвести, используя оператор `**`.

# In[ ]:


10**1.5


# In[ ]:


np.log10(1)


# В случае `log10` можно легко интерпретировать так:
# - `log10( 1 )` => 0
# - `log10( 10 )` => 1
# - `log10( 100 )` => 2
# - `log10( 1000 )` => 3
# 
# Обрати внимание, что это "показывает" количество нулей (это, конечно, упрощенная интерпретация).

# ### 7. Другие функции
# - `.qcut()` 
# - `.unique()`
# - `.nunique()`

# Иногда полезно создать "корзинки" и объединить некоторые значения, например, цены на недвижимость можно разделить на пять интервалов, от очень дешевой до очень дорогой. Для этого можно воспользоваться функцией `pd.qcut()`.

# In[ ]:


pd.qcut(df["price_num"], 5)


# Кстати, можно теперь еще применить `.value_counts()`.

# In[ ]:


pd.qcut(df["price_num"], 5).value_counts()


# Видно, что функция `.qcut()` пробовала равномерно распределить значение в этих 5 корзинках. Кстати, давай посмотрим, какие значения в этих корзинках:
# - `(1.039, 6.65]` - до 6 млн. рублей - самая дешевая недвижимость
# - `(6.65, 8.7]` - дешевая
# - `(8.7, 11.498]` - средняя
# - `(11.498, 17.68]` - дорогая
# - `(17.68, 3000.0]` - более 17 млн. рублей можно назвать очень дорогую недвижимость

# И напоследок еще стоит рассмотреть две функции: `.unique()`  и `.nunique()`. Первая возвращает список уникальных значений, а вторая - количество уникальных значений (функция `.nunique()` особенно полезна тогда, когда мы не знаем, сколько уникальных значений и если их очень много, то предоставляя их, наш браузер может "зависнуть").

# In[ ]:


df["geo_block_0"].nunique()


# Видим что в столбце `geo_block_0` у нас 9 значений - это не так уж много. Поэтому, можно их все посмотреть.

# In[ ]:


df["geo_block_0"].unique()

